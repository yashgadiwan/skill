{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6ZLpHK4Ev/hrsDWZ7kA5Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashgadiwan/skill/blob/main/skill%207%2C8%2C9%2C10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "exp 7-8"
      ],
      "metadata": {
        "id": "KnqCaPnH_Q7W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5gbNPwzT_FVD"
      },
      "outputs": [],
      "source": [
        "#pearsons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "file_path = \"/content/archive.csv\"  # Update path if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define numerical features and target column\n",
        "target_column = 'Starting_Salary'  # Replace with actual numeric target if different\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Check if the target column exists in your DataFrame\n",
        "if target_column not in df.columns:\n",
        "    raise KeyError(f\"The target column '{target_column}' is not found in the DataFrame.\")\n",
        "\n",
        "# Compute Pearson correlation for each feature with target\n",
        "correlation_results = {}\n",
        "\n",
        "print(\"\\n===== Pearson Correlation Test =====\")\n",
        "for feature in numerical_features:\n",
        "    if feature != target_column:\n",
        "        corr, _ = pearsonr(df[feature], df[target_column])\n",
        "        correlation_results[feature] = corr\n",
        "        print(f\"Feature: {feature}, Correlation: {corr}\")\n",
        "\n",
        "# Apply threshold-based filtering\n",
        "threshold = 0.2  # Change this as needed (absolute correlation threshold)\n",
        "filtered_features = {k: v for k, v in correlation_results.items() if abs(v) >= threshold}\n",
        "\n",
        "# Display filtered features\n",
        "print(\"\\n===== Features Above Correlation Threshold =====\")\n",
        "for feature, corr in filtered_features.items():\n",
        "    print(f\"Feature: {feature}, Correlation: {corr}\")\n",
        "\n",
        "# Create correlation matrix\n",
        "correlation_matrix = df[numerical_features].corr()\n",
        "\n",
        "# Display correlation matrix\n",
        "print(\"\\n===== Correlation Matrix =====\")\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gm6QTsMb_XFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOVA"
      ],
      "metadata": {
        "id": "3kmQop2K_Y-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "# Define numerical features and target variable\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "target_column = 'Job_Offers'\n",
        "\n",
        "# Ensure the target column is numeric (ANOVA requires numeric target categories)\n",
        "df[target_column] = pd.to_numeric(df[target_column])\n",
        "\n",
        "# Perform ANOVA F-Test\n",
        "f_scores, p_values = f_classif(df[numerical_features], df[target_column])\n",
        "\n",
        "# Display feature scores\n",
        "print(\"\\n===== ANOVA Feature Selection Results =====\")\n",
        "for feature, score, p_value in zip(numerical_features, f_scores, p_values):\n",
        "    print(f\"Feature: {feature}, Score: {score:.2f}, p-value: {p_value:.4f}\")\n",
        "\n",
        "# Optional: Filter features with a p-value less than 0.05 (statistically significant)\n",
        "significant_features = [feature for feature, p in zip(numerical_features, p_values) if p < 0.05]\n",
        "\n",
        "print(\"\\nStatistically Significant Features (p < 0.05):\")\n",
        "print(significant_features)"
      ],
      "metadata": {
        "id": "J4hqhBHB_ZcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHI- SQUARE"
      ],
      "metadata": {
        "id": "t7aMSXkj_eBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Define categorical features and target variable\n",
        "categorical_features = ['Gender', 'Field_of_Study', 'Current_Job_Level', 'Entrepreneurship']\n",
        "target_column = 'Job_Offers'  # Ensure this is a categorical variable\n",
        "\n",
        "# Convert categorical features into numerical using one-hot encoding\n",
        "df_encoded = pd.get_dummies(df[categorical_features])\n",
        "\n",
        "# Perform Chi-Square test using SelectKBest\n",
        "k = 5  # Number of top features to select\n",
        "selector = SelectKBest(score_func=chi2, k='all')  # Initially select all\n",
        "X_new = selector.fit_transform(df_encoded, df[target_column])\n",
        "\n",
        "# Get feature scores\n",
        "feature_scores = selector.scores_\n",
        "threshold = np.percentile(feature_scores, 50)  # Select features above median score\n",
        "\n",
        "# Get top-K features\n",
        "top_k_selector = SelectKBest(score_func=chi2, k=k)\n",
        "X_k_new = top_k_selector.fit_transform(df_encoded, df[target_column])\n",
        "selected_k_features = np.array(df_encoded.columns)[top_k_selector.get_support()]\n",
        "top_k_scores = top_k_selector.scores_[top_k_selector.get_support()]\n",
        "\n",
        "# Display results for threshold-based selection\n",
        "print(\"\\n===== Chi-Square Features (Above Threshold) =====\")\n",
        "for feature, score in zip(df_encoded.columns, feature_scores):\n",
        "    if score >= threshold:\n",
        "        print(f\"Feature: {feature}, Score: {score}\")\n",
        "\n",
        "# Display results for top-K selection\n",
        "print(\"\\n===== Top-K Chi-Square Features =====\")\n",
        "for feature, score in zip(selected_k_features, top_k_scores):\n",
        "    print(f\"Feature: {feature}, Score: {score}\")"
      ],
      "metadata": {
        "id": "bYcOercQ_eXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "i g"
      ],
      "metadata": {
        "id": "Ap01FSIb_j9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
        "\n",
        "# Define target variable\n",
        "target_column = 'Career_Satisfaction'  # Replace with actual categorical target\n",
        "\n",
        "# Encode categorical target\n",
        "df[target_column] = df[target_column].astype('category').cat.codes\n",
        "\n",
        "# Select numerical features\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Compute mutual information (information gain)\n",
        "mi_scores = mutual_info_classif(df[numerical_features], df[target_column])\n",
        "\n",
        "# Apply threshold-based filtering\n",
        "threshold = np.percentile(mi_scores, 50)  # Select features above the 50th percentile\n",
        "\n",
        "# Get top-K features\n",
        "k = 5  # Number of top features to select\n",
        "top_k_selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
        "X_k_new = top_k_selector.fit_transform(df[numerical_features], df[target_column])\n",
        "selected_k_features = np.array(numerical_features)[top_k_selector.get_support()]\n",
        "top_k_scores = top_k_selector.scores_[top_k_selector.get_support()]\n",
        "\n",
        "# Display results for threshold-based selection\n",
        "print(\"\\n===== Information Gain (Above Threshold) =====\")\n",
        "for feature, score in zip(numerical_features, mi_scores):\n",
        "    if score >= threshold:\n",
        "        print(f\"Feature: {feature}, Information Gain: {score}\")\n",
        "\n",
        "# Display results for top-K selection\n",
        "print(\"\\n===== Top-K Information Gain Features =====\")\n",
        "for feature, score in zip(selected_k_features, top_k_scores):\n",
        "    print(f\"Feature: {feature}, Information Gain: {score}\")"
      ],
      "metadata": {
        "id": "uGrf0Err_kjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "experiment 9-10"
      ],
      "metadata": {
        "id": "W26YONtH_ouG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "forward selection"
      ],
      "metadata": {
        "id": "ZwUY-nMA_tZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load data\n",
        "file_path = \"/content/archive.csv\"  # Update path if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop('Job_Offers', axis=1)\n",
        "y = df['Job_Offers']\n",
        "\n",
        "# Encode categorical columns\n",
        "for col in X.select_dtypes(include='object').columns:\n",
        "    X[col] = LabelEncoder().fit_transform(X[col])\n",
        "\n",
        "# Forward Selection\n",
        "selected_features = []\n",
        "remaining_features = list(X.columns)\n",
        "\n",
        "for _ in range(len(remaining_features)):\n",
        "    best_score = float('-inf')\n",
        "    best_feature = None\n",
        "\n",
        "    for feature in remaining_features:\n",
        "        trial_features = selected_features + [feature]\n",
        "        X_trial = sm.add_constant(X[trial_features])\n",
        "        model = sm.OLS(y, X_trial).fit()\n",
        "\n",
        "        if model.rsquared > best_score:\n",
        "            best_score = model.rsquared\n",
        "            best_feature = feature\n",
        "\n",
        "    if best_feature is not None:\n",
        "        selected_features.append(best_feature)\n",
        "        remaining_features.remove(best_feature)\n",
        "\n",
        "print(\"Selected Features (Forward Selection):\", selected_features)\n",
        "\n"
      ],
      "metadata": {
        "id": "OP_x5O3s_pH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "backward Elimination."
      ],
      "metadata": {
        "id": "xMowNs2E_wkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Start with all features\n",
        "features = list(X.columns)\n",
        "X_with_const = sm.add_constant(X[features])\n",
        "\n",
        "while len(features) > 0:\n",
        "    model = sm.OLS(y, X_with_const).fit()\n",
        "    p_values = model.pvalues[1:]\n",
        "\n",
        "    worst_p = p_values.idxmax()\n",
        "    if p_values[worst_p] > 0.05:\n",
        "        features.remove(worst_p)\n",
        "        X_with_const = sm.add_constant(X[features])\n",
        "    else:\n",
        "        break\n",
        "\n",
        "print(\"Selected Features (Backward Elimination):\", features)"
      ],
      "metadata": {
        "id": "qhL0yWtF_xMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recursive Feature Elimination (RFE):"
      ],
      "metadata": {
        "id": "TJiwAAPY_5yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor()\n",
        "rfe = RFE(model, n_features_to_select=5)\n",
        "fit = rfe.fit(X, y)\n",
        "\n",
        "selected_features = X.columns[fit.support_].tolist()\n",
        "print(\"Selected Features (RFE):\", selected_features)"
      ],
      "metadata": {
        "id": "0YveXycC_5LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross Validation (CV):"
      ],
      "metadata": {
        "id": "l8t6UTh6AAfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor()\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "print(\"Average CV Score:\", cv_scores.mean())"
      ],
      "metadata": {
        "id": "PXfDqCgWAA6j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}